[package]
name = "whis-core"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
description = "Core library for whis voice-to-text functionality"

[dependencies]
anyhow.workspace = true
tokio.workspace = true
serde.workspace = true
serde_json.workspace = true
reqwest = { workspace = true, features = ["blocking", "multipart", "json", "stream"] }
futures-util = "0.3"
cpal.workspace = true
hound = { workspace = true, optional = true }
arboard = { workspace = true, optional = true }
dotenvy.workspace = true
dirs = "5"
async-trait = "0.1"

# WebSocket for OpenAI Realtime API
tokio-tungstenite = { version = "0.26", features = ["native-tls"], optional = true }
base64 = { version = "0.22", optional = true }

# Embedded MP3 encoder for mobile (no FFmpeg dependency)
mp3lame-encoder = { version = "0.2", optional = true }

# Real-time resampling to 16kHz (always enabled - benefits all providers)
rubato = "0.15"

# Voice Activity Detection using Silero VAD model
voice_activity_detector = { version = "0.2", optional = true }

# Local Whisper transcription (whisper.cpp via whisper-rs)
whisper-rs = { version = "0.15.1", optional = true }
minimp3 = { version = "0.5", optional = true }

# Local Parakeet transcription (NVIDIA Parakeet via ONNX)
parakeet-rs = { version = "0.2", optional = true }

# Archive extraction for Parakeet model downloads
tar = { version = "0.4", optional = true }
flate2 = { version = "1.0", optional = true }

# Temp files for Parakeet audio processing
tempfile = { version = "3", optional = true }

[features]
default = ["ffmpeg", "clipboard", "local-transcription", "vad", "realtime"]
# Desktop: use FFmpeg process for audio encoding and arboard for clipboard
ffmpeg = ["hound"]
clipboard = ["arboard"]
# Mobile: use embedded mp3lame encoder (no external process needed)
embedded-encoder = ["mp3lame-encoder"]
# Local transcription (Whisper + Parakeet models)
local-transcription = ["whisper-rs", "minimp3", "parakeet-rs", "tar", "flate2", "tempfile"]
# Voice Activity Detection to skip silence during recording
vad = ["voice_activity_detector"]
# OpenAI Realtime API for streaming transcription
realtime = ["tokio-tungstenite", "base64"]
